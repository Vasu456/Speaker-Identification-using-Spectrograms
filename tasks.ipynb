{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "path='Data/Audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dropout, GlobalMaxPool2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Audio/Female\\PTDB-TUG\n",
      "120\n",
      "Data/Audio/Female\\TMIT\n",
      "205\n",
      "Data/Audio/Male\\PTDB-TUG\n",
      "120\n",
      "Data/Audio/Male\\TMIT\n",
      "64\n",
      "Data/Audio/Noizeus\\Babble\n",
      "30\n",
      "Data/Audio/Noizeus\\Car\n",
      "30\n",
      "Data/Audio/Noizeus\\NoNoise\n",
      "30\n",
      "Data/Audio/Noizeus\\Restaurant\n",
      "30\n",
      "Data/Audio/Noizeus\\Station\n",
      "30\n",
      "Data/Audio/Noizeus\\Street\n",
      "30\n",
      "Data/Audio/Noizeus\\Train\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    if not dirnames:\n",
    "        print(dirpath)\n",
    "        print(len(os.listdir(dirpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_audio_data(data_folder, output_base_directory):\n",
    "    audio_paths = []  # List to store audio file paths\n",
    "    class_labels = []  # List to store class labels\n",
    "    count = 0\n",
    "\n",
    "    def generate_spectrogram(audio_path, output_directory):\n",
    "        nonlocal count\n",
    "        try:\n",
    "            # Read audio file\n",
    "            sample_rate, audio_data = wavfile.read(audio_path)\n",
    "\n",
    "            # Generate spectrogram\n",
    "            Pxx, freqs, bins, im = plt.specgram(audio_data, NFFT=1024, Fs=8000, noverlap=900)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            # Save the spectrogram image\n",
    "            output_filename = os.path.join(output_directory, os.path.splitext(os.path.basename(audio_path))[0] + '.png')\n",
    "            plt.savefig(output_filename, bbox_inches='tight')\n",
    "            plt.close()  # Close the figure to release resources\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error during spectrogram computation for {audio_path}: {ve}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            audio_path = os.path.join(root, file)\n",
    "\n",
    "            # Check if the file is a WAV file\n",
    "            if audio_path.lower().endswith('.wav'):\n",
    "                # Use the relative path from data_folder as the class label\n",
    "                class_label = os.path.relpath(root, data_folder)\n",
    "                audio_paths.append(audio_path)\n",
    "                class_labels.append(class_label)\n",
    "\n",
    "                # Generate spectrogram\n",
    "                output_directory = os.path.join(output_base_directory, class_label)\n",
    "                generate_spectrogram(audio_path, output_directory)\n",
    "\n",
    "                count += 1\n",
    "                print(count, end=',')\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame({'Path': audio_paths, 'Class': class_labels})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1160.wav: only 1-dimensional arrays can be used\n",
      "130,131,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1208.wav: only 1-dimensional arrays can be used\n",
      "132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1790.wav: only 1-dimensional arrays can be used\n",
      "157,158,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1838.wav: only 1-dimensional arrays can be used\n",
      "159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI578.wav: only 1-dimensional arrays can be used\n",
      "186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX128.wav: only 1-dimensional arrays can be used\n",
      "214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX308.wav: only 1-dimensional arrays can be used\n",
      "263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX38.wav: only 1-dimensional arrays can be used\n",
      "286,287,288,289,290,291,292,293,294,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX398.wav: only 1-dimensional arrays can be used\n",
      "295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,Error during spectrogram computation for Data/Audio/Male\\TMIT\\SX430.wav: only 1-dimensional arrays can be used\n",
      "498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_sa2.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si454.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si473.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si502.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si523.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Path            Class\n",
       "0    Data/Audio/Female\\PTDB-TUG\\mic_F01_sa2.wav  Female\\PTDB-TUG\n",
       "1  Data/Audio/Female\\PTDB-TUG\\mic_F01_si454.wav  Female\\PTDB-TUG\n",
       "2  Data/Audio/Female\\PTDB-TUG\\mic_F01_si473.wav  Female\\PTDB-TUG\n",
       "3  Data/Audio/Female\\PTDB-TUG\\mic_F01_si502.wav  Female\\PTDB-TUG\n",
       "4  Data/Audio/Female\\PTDB-TUG\\mic_F01_si523.wav  Female\\PTDB-TUG"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_folder = 'Data/Audio/'\n",
    "output_base_directory = 'spectrograms_/'\n",
    "\n",
    "df_result = process_audio_data(data_folder, output_base_directory)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spec_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        spec_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode=\"categorical\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        spec_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=16,\n",
    "        class_mode=\"categorical\",\n",
    "        subset=\"validation\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(32, (3, 3), input_shape=input_size, activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        GlobalMaxPool2D()\n",
    "    ])\n",
    "    return model\n",
    "def build_model(input_size):\n",
    "    input_layer = Input(input_size)\n",
    "    encoder = build_encoder(input_size)(input_layer)\n",
    "    output_layer = Dense(3, activation='softmax')(encoder)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 576 images belonging to 3 classes.\n",
      "Found 143 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.0846 - accuracy: 0.4132 - val_loss: 1.0460 - val_accuracy: 0.4545\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.9018 - accuracy: 0.5799 - val_loss: 0.6593 - val_accuracy: 0.7273\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.6960 - accuracy: 0.6719 - val_loss: 0.6024 - val_accuracy: 0.7413\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.6347 - accuracy: 0.6979 - val_loss: 0.5311 - val_accuracy: 0.7413\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.6976 - accuracy: 0.6719 - val_loss: 0.9568 - val_accuracy: 0.4615\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.7453 - accuracy: 0.6701 - val_loss: 0.6220 - val_accuracy: 0.7413\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 34s 2s/step - loss: 0.6379 - accuracy: 0.6927 - val_loss: 0.5585 - val_accuracy: 0.7483\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 34s 2s/step - loss: 0.6306 - accuracy: 0.6944 - val_loss: 0.6605 - val_accuracy: 0.7063\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 36s 2s/step - loss: 0.6605 - accuracy: 0.6927 - val_loss: 0.5439 - val_accuracy: 0.7483\n",
      "9/9 [==============================] - 2s 229ms/step - loss: 0.5311 - accuracy: 0.7413\n",
      "Evaluation Result: [0.5311135649681091, 0.7412587404251099]\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_generator, val_generator):\n",
    "    checkpoint_filepath = 'model/ckpt/checkpoint.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=[early])\n",
    "\n",
    "    return model\n",
    "def evaluate_model(model, val_generator):\n",
    "    return model.evaluate(val_generator)\n",
    "\n",
    "# Load data\n",
    "spec_dir = \"spectrogram/\"\n",
    "train_generator, val_generator = load_data(spec_dir)\n",
    "\n",
    "# Build model\n",
    "input_size = train_generator.image_shape\n",
    "model = build_model(input_size)\n",
    "\n",
    "# Compile model\n",
    "compiled_model = compile_model(model)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(compiled_model, train_generator, val_generator)\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_result = evaluate_model(trained_model, val_generator)\n",
    "print(\"Evaluation Result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def process_noisy_audio_data(data_folder, output_base_directory):\n",
    "    audio_paths = []  # List to store audio file paths\n",
    "    class_labels = []  # List to store class labels\n",
    "    count = 0\n",
    "\n",
    "    def generate_noisy_spectrogram(audio_path, output_directory):\n",
    "        nonlocal count\n",
    "        try:\n",
    "            # Read audio file\n",
    "            sample_rate, audio_data = wavfile.read(audio_path)\n",
    "\n",
    "            # Add random noise to the audio\n",
    "            noise = np.random.normal(0, 0.001, audio_data.shape[0])\n",
    "            noisy_wave = audio_data + noise\n",
    "\n",
    "            # Generate spectrogram\n",
    "            Pxx, freqs, bins, im = plt.specgram(noisy_wave, NFFT=1024, Fs=8000, noverlap=900)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            # Save the spectrogram image\n",
    "            output_filename = os.path.join(output_directory, os.path.splitext(os.path.basename(audio_path))[0] + '.png')\n",
    "            plt.savefig(output_filename, bbox_inches='tight')\n",
    "            plt.close()  # Close the figure to release resources\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error during spectrogram computation for {audio_path}: {ve}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            audio_path = os.path.join(root, file)\n",
    "\n",
    "            # Check if the file is a WAV file\n",
    "            if audio_path.lower().endswith('.wav'):\n",
    "                # Use the relative path from data_folder as the class label\n",
    "                class_label = os.path.relpath(root, data_folder)\n",
    "                audio_paths.append(audio_path)\n",
    "                class_labels.append(class_label)\n",
    "\n",
    "                # Generate noisy spectrogram\n",
    "                output_directory = os.path.join(output_base_directory, class_label)\n",
    "                generate_noisy_spectrogram(audio_path, output_directory)\n",
    "\n",
    "                count += 1\n",
    "                print(count, end=',')\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame({'Path': audio_paths, 'Class': class_labels})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1160.wav: operands could not be broadcast together with shapes (232848,2) (232848,) \n",
      "130,131,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1208.wav: operands could not be broadcast together with shapes (136040,2) (136040,) \n",
      "132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1790.wav: operands could not be broadcast together with shapes (102736,2) (102736,) \n",
      "157,158,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI1838.wav: operands could not be broadcast together with shapes (129549,2) (129549,) \n",
      "159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SI578.wav: operands could not be broadcast together with shapes (157773,2) (157773,) \n",
      "186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX128.wav: operands could not be broadcast together with shapes (111204,2) (111204,) \n",
      "214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX308.wav: operands could not be broadcast together with shapes (156643,2) (156643,) \n",
      "263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX38.wav: operands could not be broadcast together with shapes (165110,2) (165110,) \n",
      "286,287,288,289,290,291,292,293,294,Error during spectrogram computation for Data/Audio/Female\\TMIT\\SX398.wav: operands could not be broadcast together with shapes (171885,2) (171885,) \n",
      "295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,Error during spectrogram computation for Data/Audio/Male\\TMIT\\SX430.wav: operands could not be broadcast together with shapes (229744,2) (229744,) \n",
      "498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_sa2.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si454.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si473.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si502.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/Audio/Female\\PTDB-TUG\\mic_F01_si523.wav</td>\n",
       "      <td>Female\\PTDB-TUG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Path            Class\n",
       "0    Data/Audio/Female\\PTDB-TUG\\mic_F01_sa2.wav  Female\\PTDB-TUG\n",
       "1  Data/Audio/Female\\PTDB-TUG\\mic_F01_si454.wav  Female\\PTDB-TUG\n",
       "2  Data/Audio/Female\\PTDB-TUG\\mic_F01_si473.wav  Female\\PTDB-TUG\n",
       "3  Data/Audio/Female\\PTDB-TUG\\mic_F01_si502.wav  Female\\PTDB-TUG\n",
       "4  Data/Audio/Female\\PTDB-TUG\\mic_F01_si523.wav  Female\\PTDB-TUG"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_noisy = 'Data/Audio/'\n",
    "output_base_directory_noisy = 'noisy_spectrograms/'\n",
    "\n",
    "df_result_noisy = process_noisy_audio_data(data_folder_noisy, output_base_directory_noisy)\n",
    "df_result_noisy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spec_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        spec_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode=\"categorical\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        spec_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=16,\n",
    "        class_mode=\"categorical\",\n",
    "        subset=\"validation\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(32, (3, 3), input_shape=input_size, activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        GlobalMaxPool2D()\n",
    "    ])\n",
    "    return model\n",
    "def build_model(input_size):\n",
    "    input_layer = Input(input_size)\n",
    "    encoder = build_encoder(input_size)(input_layer)\n",
    "    output_layer = Dense(3, activation='softmax')(encoder)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 576 images belonging to 3 classes.\n",
      "Found 143 images belonging to 3 classes.\n",
      "Epoch 1/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.0756 - accuracy: 0.4444 - val_loss: 1.0699 - val_accuracy: 0.4545\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.0751 - accuracy: 0.4514 - val_loss: 1.0734 - val_accuracy: 0.4545\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.0718 - accuracy: 0.4514 - val_loss: 1.0710 - val_accuracy: 0.4545\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.0615 - accuracy: 0.4514 - val_loss: 1.0619 - val_accuracy: 0.4545\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.0499 - accuracy: 0.4514 - val_loss: 1.0463 - val_accuracy: 0.4545\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.8416 - accuracy: 0.5451 - val_loss: 0.5102 - val_accuracy: 0.8601\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.6655 - accuracy: 0.7135 - val_loss: 0.5083 - val_accuracy: 0.8811\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.4182 - accuracy: 0.8472 - val_loss: 0.3173 - val_accuracy: 0.9091\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.3189 - accuracy: 0.8837 - val_loss: 0.1509 - val_accuracy: 0.9790\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 0.3402 - accuracy: 0.8715 - val_loss: 0.1231 - val_accuracy: 0.9930\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 0.2627 - accuracy: 0.9045 - val_loss: 0.0829 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 37s 2s/step - loss: 0.2344 - accuracy: 0.9149 - val_loss: 0.1676 - val_accuracy: 0.9580\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.1935 - accuracy: 0.9358 - val_loss: 0.1745 - val_accuracy: 0.9650\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 34s 2s/step - loss: 0.1977 - accuracy: 0.9288 - val_loss: 0.1309 - val_accuracy: 0.9720\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.1644 - accuracy: 0.9358 - val_loss: 0.1576 - val_accuracy: 0.9580\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.1521 - accuracy: 0.9514 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.1150 - accuracy: 0.9549 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 0.0790 - accuracy: 0.9722 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 0.0731 - accuracy: 0.9774 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 32s 2s/step - loss: 0.0587 - accuracy: 0.9809 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Evaluation Result: [0.00878223031759262, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_generator, val_generator):\n",
    "    checkpoint_filepath = 'model/ckpt_noise/checkpoint.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=[early])\n",
    "\n",
    "    return model\n",
    "def evaluate_model(model, val_generator):\n",
    "    return model.evaluate(val_generator)\n",
    "\n",
    "# Load data\n",
    "spec_dir = \"noisy_spectrogram/\"\n",
    "train_generator, val_generator = load_data(spec_dir)\n",
    "\n",
    "# Build model\n",
    "input_size = train_generator.image_shape\n",
    "model = build_model(input_size)\n",
    "\n",
    "# Compile model\n",
    "compiled_model = compile_model(model)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(compiled_model, train_generator, val_generator)\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_result = evaluate_model(trained_model, val_generator)\n",
    "print(\"Evaluation Result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.h6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.h6\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 240ms/step - loss: 0.0088 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00878223031759262, 1.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=tf.keras.models.load_model('model.h6')\n",
    "k.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
